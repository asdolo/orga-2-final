# Resumen Final Orga2

[Versi√≥n PDF (recomendado)](https://docs.google.com/viewer?url=https://raw.githubusercontent.com/asdolo/orga-2-final/master/Resumen_Final_Orga2.pdf)

---

# Tecnolog√≠a de integraci√≥n

### Escenario actual

- Queremos achicar los nan√≥metros que ocupa un transistor
- En 2012 ten√≠amos 22nm
- Actualmente se trabaja en lograr 7 y 5nm.
- **Y luego? Violamos las leyes de la f√≠sica. No podemos achicar m√°s.**

### Ley de Moore

El n√∫mero de transistores que incorporemos en un chip aproximadamente se duplicar√° cada 24 meses.

### Evoluci√≥n tecnol√≥gica

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled.png)

Los progresos en scaling muchas veces obedecen a mejoras en el proceso de fabricaci√≥n, pero cada tanto se produce alg√∫n salto cu√°ntico en esto, que es cuando aparecen tecnolog√≠as de fabricaci√≥n de semiconductores disruptivas.

- Strained Silicon
- High-k Metal Gate
- Tri-Gate:
    
    Ademas del scaling, disminuye mucho el consumo. En 2011 el consumo de energ√≠a es una preocupaci√≥n muy fuerte.
    

**Velocidad de conmutaci√≥n:** Es la velocidad que tarda en la que un transistor pasa de 0 a 1. **Evoluciona en funci√≥n del scaling: cuanto m√°s chico es el transistor, m√°s alta es la velocidad de conmutaci√≥n.**

### Tendencias tecnol√≥gicas

La tarea de un dise√±ador est√° inevitablemente influida por el rumbo de las tecnolog√≠as.

1. La densidad de transistores por unidad de superficie aumenta 35% por a√±o en promedio (Ley de Moore alternativa)
2. El tama√±o del die (disco con circuitos integrados) aumenta de 10 a 20% por a√±o. Esto deriva en un crecimiento en la cantidad de transistores de entre 40% y 55% de un a√±o a otro.
3. La velocidad de clock ya no crece, Parecer√≠a haber alcanzado un techo en los 3 GHz aproximadamente.
4. La capacidad de almacenamiento de las memorias DRAM crece a raz√≥n de un 40% por a√±o.
5. Los discos r√≠gidos aumentan su capacidad 25% a 30% por a√±o. Su costo por bit de almacenamiento se mantiene entre 50 y 100 veces por debajo del costo de un bit de memoria DRAM.

# Arquitectura de Computadores

## Arquitectura vs Microarquitectura

### Arquitectura

Es el conjunto de recursos que nosotros tenemos como programadores para poder trabajar y poder desarrollar programas. **Se mantiene constante de un modelo a otro (en general).**

- Registros
- Set de instrucciones
- Estructuras de memoria (descriptores de segmento y de p√°gina)

### Micro Arquitectura

Es la implementaci√≥n es el silicio de la arquitectura. Lo que est√° detr√°s del set de instrucciones. Determina de qu√© manera se ejecutan las instrucciones, cu√°n r√°pido o no. Cuestiones que est√°n relacionadas a electr√≥nica, tecnolog√≠a, materiales, etc. Un procesador con una misma arquitectura tenga una estructura muy s√≥lida, robusta, performante, y otro procesador con la misma arquitectura pero con una organizaci√≥n o microarquitectura diferente por debajo sea un procesador menos performante, m√°s econ√≥nico, m√°s lento, que consuma menos. **Cambian de un modelo a otro. Es "levantar el cap√≥ y ver qu√© hay debajo de la arquitectura".**

<aside>
üí° IA-32 es una **arquitectura**. Se inicia con el procesador 80386 en 1985 y llega hasta los procesadores Intel Core i7, i5, i3, ATOM y Xeon.

En el camino han pasado diferentes generaciones de **Microarquitectura** para m√°s de 25 modelos de procesadores.

</aside>

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%201.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%201.png)

<aside>
üí° **Microarquitectura = Organizaci√≥n + Hardware**

</aside>

## Organizaci√≥n

Se refiere a los detalles de implementaci√≥n de la ISA (Instruction Set Architecture).

- Organizaci√≥n e interconexi√≥n de la memoria.
- Dise√±o de los diferentes bloques de la CPU que van a ejecutar la ISA. Qu√© caracter√≠sticas van a tener. Qu√© grado de paralelismo va a tener. Puede ejecutar varias instrucciones a la vez, o no? Puede ejecutar instrucciones fuera de orden o no?
- Implementaci√≥n de paralelismo a nivel de instrucciones, y/o de datos.
- Podemos encontrar procesadores que poseen la misma arquitectura pero organizaci√≥n diferente **(ejemplo: AMD FX e Intel Core i7 tienen la misma ISA, pero su organizaci√≥n es completamente diferente).**

## Hardware

Se refiere a cosas todav√≠a m√°s "de abajo".

- Dise√±o l√≥gico y tecnolog√≠a de fabricaci√≥n.
- Se pueden tener procesadores que tengan la misma ISA, la misma organizaci√≥n y distinto hardware.

## Por qu√© necesitamos saber todo esto?

- Comprender qu√© hay debajo del software.
- Comprender c√≥mo el dise√±o de un hardware en particular impacta nuestra tareas como programadores.

# Sistema de Memoria

## Jerarqu√≠as de Memorias

### **Baja latencia o alta capacidad**

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%202.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%202.png)

- Cuanto m√°s cerca de la c√∫spide est√°s, ten√©s memorias m√°s veloces pero a la vez de menor capacidad, porque su costo por bit aumenta a medida que subimos.
- Queremos que nuestros datos est√©n lo m√°s cerca posible de la CPU.
- Lo que tenemos m√°s cercano al procesador son los registros.

## Principio de Vecindad o Localidad

### Principio de funcionamiento

El comportamiento de los algoritmos de software que se emplean habitualmente, es el que rige c√≥mo y para qu√© se dise√±a el hardware. Muchas de las cosas que se dise√±a en hardware no son inventos, son simplemente observaciones de c√≥mo se comportamiento y, entonces, buscar soluciones para agilizar ese comportamiento.

<aside>
üí° **Principio de vecindad temporal**

Una direcci√≥n de memoria que est√° siendo accedida actualmente tiene muy alta probabilidad de seguir siendo accedida en el futuro inmediato.

</aside>

<aside>
üí° **Principio de vecindad espacial**

Si se est√° accediendo a una direcci√≥n determinada de memoria actualmente, la probabilidad de que esta direcci√≥n y sus direcciones vecinas sean accedidas en el futuro inmediato es muy alta.

</aside>

# Tecnolog√≠as de Memoria

## Clasificaci√≥n de memorias

### **Memorias No Vol√°tiles (ROM)**

- Retienen la informaci√≥n almacenada cuando se les desconecta la alimentaci√≥n.
- Han evolucionado tecnol√≥gicamente, desde ROM que se grababan en la fabrica hasta ROMs borrables y reprogramables el√©ctricamente.

### **Memorias Vol√°tiles (RAM)**

- Pierden la informaci√≥n almacenada cuando se les desconecta la alimentaci√≥n.
- Pueden almacenar mayores cantidades de informaci√≥n y modificarla en tiempo real mucho m√°s r√°pido que las No Vol√°tiles.
- Se clasifican, de acuerdo con la tecnolog√≠a y dise√±o interno, en din√°micas (DRAM) y est√°ticas (SRAM).

### **Uso en un computador**

- La memoria no vol√°til se usa fundamentalmente para almacenar el programa de arranque de cualquier sistema.
- Se conectan en un espacio de direcciones determinado por el propio microprocesador de acuerdo a la direcci√≥n en la que √©ste ir√° a buscar la primer instrucci√≥n luego de encender el equipo.
- El resto es RAM y all√≠ el sistema copia incluso buena parte del c√≥digo de arranque para que se ejecute m√°s r√°pido (recordemos que las memorias vol√°tiles tienen menor tiempo de acceso)

### **Memorias RAM din√°micas (DRAM)**

Tenemos una celda de memoria representada por un transistor y un capacitor.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%203.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%203.png)

- Almacena la informaci√≥n en forma de estado de carga en un capacitor y la sostiene durante un breve lapso con la ayuda de un transistor.
- Una celda (un bit) se implementa con un s√≥lo transistor ‚Üí m√°xima capacidad de almacenamiento por CI.
- Ese transistor est√° generalmente en estado de Corte. Consume m√≠nima energ√≠a.
- Al leer el bit, se descarga la capacidad (lectura destructiva), con lo cual necesita regenerar la carga cada vez que se la lee.
- Se descargan solas por las corrientes de fuga de los transistores, con lo cual hay que refrescarlas manualmente periodicamente. La recarga se hace haciendo una lectura "dummy" de toda una fila a la vez.

### **Memorias RAM est√°ticas (SRAM)**

- Tienen 6 transistores por cada bit. Es decir, la densidad es peor que en la RAM din√°mica.
- 3 de los 6 transistores est√°n cerrados, con lo cual consumen mucha corriente, con lo cual cada celda consume mucha energ√≠a.
- No se destruye la informaci√≥n al leer un bit.
- La velocidad de lectura es casi instant√°nea.

## Memorias y velocidad del Procesador

### **Conexion b√°sica (Seg√∫n Von Newmann)**

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%204.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%204.png)

Intel presenta su arquitectura muy parecida a la de Von Neumann. El 8088 ten√≠a una l√≠nea de control llamada `Ready` que serv√≠a para saber cu√°ndo una memoria estaba lista para leer o escribir un dato. Cuando los procesadores aumentaron su velocidad, las RAM se quedaron atr√°s.

El problema consiste en decidir qu√© tipo de RAM usar en el sistema. Hay dos opciones:

- **RAM din√°mica (DRAM)**
    - Consumo m√≠nimo.
    - Capacidad de almacenamiento comparativamente alta (1 transistor por bit).
    - Costo por bit bajo.
    - Tiempo de acceso alto (lento), debido al circuito de regeneraci√≥n de carga.
- **RAM est√°tica (SRAM)**
    - Alto consumo relativo.
    - Capacidad de almacenamiento comparativamente baja (6 transistores por bit).
    - Costo por bit alto (por ser menos densa).
    - Tiempo de acceso bajo (es m√°s r√°pida).

<aside>
üí° **Conclusi√≥n:**

No podemos poner un banco de RAM est√°tica porque el costo y consumo es muy elevado. Entonces, elegimos usar RAM din√°mica, aunque al ser lentas no podemos aprovechar la velocidad superior del procesador (cuello de botella). **Esto √∫ltimo se resolvi√≥ con la memoria cache.**

</aside>

# Memoria Cache

## Principio de Funcionamiento

Es un banco de **RAM est√°tica** que contiene una copia de los datos e instrucciones que est√°n en la memoria principal y el procesador necesita en este momento. La dificultad est√° en lograr que esta copia est√© disponible justo cuando el procesador la necesita permiti√©ndole acceder a estos √≠tems sin utilizar *wait states*.

Este banco de RAM est√°tica tiene que ser peque√±o para no comprometer el costo y consumo de nuestro sistema (mismo problema de antes), pero a la vez tiene que ser suficientemente grande para contener siempre lo que el procesador necesita.

Esta memoria cache de peque√±o tama√±o combinada con una gran cantidad de memoria DRAM donde almacenemos el resto de las cosas resuelve el problema mediante una t√≠pica soluci√≥n de compromiso.

Qui√©n se encarga de que las cosas que est√°n en la DRAM que el procesador va a utilizar est√©n en la cache? Esto cuesta hardware. Tiene que haber un controlador que intermedie entre el procesador y las dos memorias para asegurar que eso est√© completamente funcional en el momento que el procesador lo necesite. **A este se lo denomina Controlador cache.**

### Caracter√≠sticas y m√©tricas

**Se maneja con los principios de vecindad.** Al principio el cache esta vac√≠o y el sistema es ineficiente porque tiene que ir a buscar todo a la RAM. Cuando se va a buscar algo a la RAM, el controlador la guarda tambi√©n en la cache. Si se va a buscar una instrucci√≥n, el controlador busca esa instrucci√≥n y m√°s, ya que seguro se van a utilizar las siguientes instrucciones.

Para medir la eficiencia de esto, se establecieron m√©tricas. La operaci√≥n de memoria es un **hit** o un **miss** seg√∫n encuentre o no encuentre lo que busca en el cache (cuando lo encuentra es un **hit**, y cuando no lo encuentra es un **miss**). El **hitrate** se mide con la relaci√≥n entre la **cantidad de hit** sobre la **cantidad de accesos totales**.

![**Queremos que el hitrate tienda a 1.**](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%205.png)

**Queremos que el hitrate tienda a 1.**

## Controlador de cache

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%206.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%206.png)

Quien maneja el BUS de control es quien maneja el BUS del sistema. **Este BUS ahora es manejado por el Controlador de Cache. El Controlador de Cache es el master del BUS de control. La idea es que intercepta los pedidos del procesador y se los manda o bien a la Memoria cache o bien a la memoria RAM seg√∫n haya un hit o miss.**

<aside>
üí° El BUS de control es un bus con l√≠neas de control del tipo **Read**, **Write**, etc.

</aside>

### Operaci√≥n de acceso a memoria para lectura

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%207.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%207.png)

1. El procesador inicia un ciclo de lectura. Pone la direcci√≥n de memoria en el BUS de address y **activa la l√≠nea Memory Read en el BUS de control**.
2. El controlador de Cache toma la l√≠nea de `Memory Read` (pues maneja el BUS de control) y dice, "Ok, el procesador quiere leer". Tambi√©n le llega el BUS de address y entonces puede saber qu√© direcci√≥n quiere leer. El controlador de Cache mira si la direcci√≥n de memoria est√° guardada en la cache (usa el directorio interno). **Sabe si tiene un hit o un miss.**
    
    a. Si tiene un **hit**, le manda el `Memory Read` a la cache en vez de enviarselo a la memoria RAM. La cache responde el dato por el BUS de data y el procesador lo lee. **Fin.**
    
    b.
    
    1. Si tiene un **miss**, le manda el `Memory Read` a la RAM. La memoria RAM se toma su tiempo para acceder al dato y luego mete el dato en el BUS de datos.
    2. Adem√°s, el controlador de cache le dice a la memoria cache que un dato va a ser escrito (seteando la linea `Memory Write` en su BUS de control) y le manda la misma data que devolvi√≥ la RAM en el punto anterior. Este dato lo guarda para honrar el principio de vecindad temporal.
    3. Luego, el controlador de cache actualiza la direcci√≥n en su directorio de cache.
    4. Por √∫ltimo, el mismo controlador de cach√© le devuelve el dato al procesador.

## Organizaci√≥n de un cache

### L√≠neas

El controlador cach√© no ve a la memoria principal como una sucesi√≥n de bytes, sino como una sucesi√≥n de l√≠neas. **La m√≠nima unidad de memoria para un controlador cache es la l√≠nea.** La l√≠nea est√° compuesta de una cantidad *b* de bytes. **Esto est√° pensado as√≠ para aprovechar el principio de vecindad espacial.** El controlador de cach√© se trae una l√≠nea completa en vez de un √∫nico byte.

**Tag**

A cada l√≠nea dentro del cach√© se le asigna una etiqueta (tag). El tag indica la posici√≥n de memoria, en la DRAM, en la cual comienza la l√≠nea. Dentro del cache las l√≠neas no necesariamente est√°n en el mismo orden en el que aparecen en memoria.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%208.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%208.png)

### Sistema Cache de Mapeo Directo

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%209.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%209.png)

- La DRAM se divide en bloques del mismo tama√±o de la cache, los cuales se referencian con los bits m√°s significativos de la direcci√≥n.
- Cada bloque se divide en sets.
- Cada set se divide en l√≠neas. Generalmente 8 l√≠neas.
- El directorio cache almacena, para cada set, el n√∫mero de bloque (tag) que est√° guardado en la cache. Adem√°s tiene un bit de validez general del tag y un bit de validez individual de la l√≠nea.

**Funcionamiento:**

1. Particiono la direcci√≥n.
    
    ![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2010.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2010.png)
    
2. Accedo al n√∫mero de set en el Directorio Cache.
3. Comparo el n√∫mero de banco en la direcci√≥n con el tag almacenado en la entrada del directorio cache.
    - **Si el tag del directorio cache NO coincide con el n√∫mero de banco de la direcci√≥n, miss.**
4. **Si el bit de validez general del tag est√° apagado, miss.**
5. Accedo al bit de validez individual de la l√≠nea.
    1. **Si el bit de validez de la l√≠nea est√° apagado, miss.**
6. **Devuelvo hit.**

**Problema:** **El mapeo directo es el menos eficiente de todos.** Para que sea eficiente, todas las l√≠neas del set deben ser del mismo banco.

**Soluci√≥n:** Sistema de Cache asociativo de n v√≠as

### Sistema Cache Asociativo de 2 V√≠as

Este es el que se usa en la pr√°ctica.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2011.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2011.png)

Ahora la cache se divide en 2 v√≠as o bancos. Ahora con dos v√≠as puedo tener el mismo n√∫mero de set de dos bancos de DRAM distintos. **Cuantas m√°s v√≠as tengo, m√°s sets con el mismo n√∫mero y distinto bancos puedo tener.** La eficiencia de esto es muy alta. **Con un cache asociativo de entre 4 y 8 v√≠as se logran eficiencias superiores al 90%.**

Los bits LRU sirven para marcar la v√≠a menos recientemente usada. Sirve para saber a cu√°l de las dos v√≠as desalojar en caso que quiera almacenar un set de un tercer banco. De nuevo, esto sigue el principio de vecindad temporal.

## Coherencia de un cache

### ¬øQu√© ocurre durante las escrituras?

Cuando una variable est√° alojada en la cache, tambi√©n est√° alojada en alguna direcci√≥n de la DRAM. Idealmente, ambas copias de la variable tendr√≠an que mantener el mismo valor. Esto es lo que se entiende por coherencia. Cuando un procesador modifica la variable y escribe en un s√≥lo lugar, se dice que se perdi√≥ la condici√≥n coherencia de esa variable. Hay que definir qu√© pol√≠tica de escritura vamos a utilizar.

### Pol√≠ticas de escritura

**Write through:**

El procesador va y escribe en la DRAM, al costo de tiempo que sea. Luego, el controlador cache, una vez que esa escritura se efectu√≥, refresca el dato en el cach√© para mantenerlo actualizado. **Con esto la coherencia es perfecta, pero la desventaja ahora es que el tiempo de escritura me queda siempre penalizado por  la demora de la DRAM. O sea, para las escrituras, la cache no cumple su cometido. Es lo mismo que no tenerlo.**

**Write through buffered:**

Es write through mejorado con un buffer. Cuando el procesador quiere escribir, en realidad escribe en el cache. Luego, el controlador cache actualiza el dato en la DRAM m√°s tarde. El buffer en el controlador de cache se utiliza para encolar las operaciones de escritura a memoria. **Este m√©todo no penaliza tanto las escrituras, salvo que el procesador se ponga a escribir demasiado frecuentemente.**

**Write back o copy back:**

El procesador escribe en la cache. El controlador de cache marca l√≠nea escrita como sucia (dirty). La cache queda completamente incoherente de la copia en DRAM. Luego, cuando la l√≠nea tiene que ser desalojada, va y copia su valor en la DRAM. **Esta pol√≠tica no mantiene coherencia pero es muy performante ya que el procesador siempre escribe en la cache.**

### Coherencia en sistemas SMP

En un sistema multi-procesador, la pol√≠tica de write back es, en principio, inviable. Uno quiere mantener la coherencia lo m√°s posible, as√≠ que utilizar√≠a write thorugh o write through buffered.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2012.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2012.png)

<aside>
üí° **SMP = Symmetric Multi-Processing**

Se dice sim√©trico porque los procesador son iguales, son sim√©tricos.

</aside>

**Escenario:**

- Tenemos dos thread de un mismo proceso ejecutando en los dos procesadores con las mismas variables y justo tienen la misma variable cada uno en su cache.
- Uno de los dos thread modifica la variable en cache, con lo cual, el dato queda incoherente para la cache del otro procesador y para la DRAM.
- **Problema: Ni la DRAM ni la otra cache se enteran que la variable se modific√≥.**

Independientemente de la pol√≠tica que utilicemos, tenemos que restaurar la coherencia lo m√°s r√°pido posible. Lo m√°s dificil es que el otro procesador se entere del cambio.

**Soluci√≥n:** **Agregamos el Snoop BUS ("BUS de espiar").**

### Snoop BUS

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2013.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2013.png)

El procesador 1 escribi√≥ una variable. La variable se refresc√≥ tambi√©n en la DRAM (asumiendo que utilizamos write through o write through buffered). Esto quiere decir que por el BUS del sistema viaj√≥ la variable. Con el Snoop BUS, los controladores de cache esp√≠an el BUS de address y el BUS de control dentro del BUS del sistema para saber qu√© hacen los dem√°s. Entonces, cuando un procesador detecta una direcci√≥n que se est√° escribiendo por otro procesador, marca la direcci√≥n como inv√°lida en su memoria cache, as√≠ la pr√≥xima vez la va a buscar a la DRAM. **El Snoop BUS resuelve el problema de la coherencia.**

<aside>
‚ö†Ô∏è **El Snoop BUS no es BUS entre los controladores. No une a los controladores. Une a cada controlador con el BUS de address del sistema, pero en sentido inverso.**

</aside>

**Siguiente motivaci√≥n:**

Si bien el Snoop BUS resuelve el problema de coherencia, nos gustar√≠a hacer optimizaciones. **Queremos ver si existe alguna forma de usar Copy Back, ya que es la mejor pol√≠tica de escritura. Si bien podr√≠amos usar Write Through Buffered siempre, y utilizar siempre la cache, de todas formas estar√≠amos utilizando el Bus del Sistema todo el tiempo cuando a no es necesario. En el escenario que vimos hasta ahora, copy back no es factible, ya que depende de que la memoria DRAM est√© siempre luego de escribir.** Deber√≠amos buscar una forma de utilizar copy back todo lo que se pueda, y cuando no queda m√°s remedio, utilizar write through o write through buffered.

- Cuando un procesador escribe un dato que est√° en otra cache, copy back no sirve.
- Pero cuando s√≥lo ese procesador tiene esa variable, no hace falta actualizarlo en la DRAM inmediatamente (para qu√© la va a estar actualizando en la DRAM del sistema si no la tiene nadie m√°s?).
- Si el procesador pudiera saber si alguien m√°s tiene esa variable, podr√≠a hacer copy back mientras solo √©l la utilice.

### Protocolo MESI

<aside>
üí° **M.E.S.I. = Modified, Exclusive, Shared, Invalid**

Son los cuatro estados que puede tomar cada l√≠nea de cache en un controlador.

</aside>

**`Modified`:**

Esta es la √∫nica copia de la l√≠nea en una cach√© pero la modifiqu√©. La tengo dirty. **Esto me permite hacer copy back.** **En este estado tengo la certeza de que soy el √∫nico poseedor de la variable.**

**`Exclusive`**:

Esta es la √∫nica copia de la l√≠nea en una cach√© y adem√°s est√° limpia (sin modificar). **En este estado tengo la certeza de que soy el √∫nico poseedor de la variable.**

**`Shared`:**

No soy el √∫nico que la tiene. **Puede** estar almacenada en los caches de otros procesadores. **No puedo hacer copy back. Hago write through y que los dem√°s utilicen el Snoop BUS.**

**`Invalid`:**

La l√≠nea de cache no es v√°lida. Es basura.

### Coherencia en sistemas SMP con MESI

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2014.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2014.png)

**Se agregan las l√≠neas Shared y RFO (Request For Ownership).**

### Operaci√≥n del Protocolo MESI

- Todas las transacciones a l√≠neas que un procesador quiere ejecutar (ya sea una lectura o escritura de l√≠nea) se resuelven en el cache, excepto si la l√≠nea en el cache est√° marcada como `Invalid`.
- Las l√≠neas marcadas como `Invalid` se buscan en DRAM.
- El controlador cache que posee l√≠neas en estado `Exclusive`, monitorea siempre por el Snoop BUS cada transaccion en la DRAM.
    - Si detecta un acceso a una l√≠nea que tiene almacenada en su cache, el controlador owner cambia su estado a `Shared` y activa la l√≠nea Shared para enviar un broadcast al resto. Todos se enteran que alguien tiene esa l√≠nea shared.
- Una l√≠nea que est√° en estado `Shared` o en estado `Exclusive` puede ser descartada y pasar a inv√°lida en cualquier momento sin avisarle a nadie.
- Una l√≠nea `Modified` tambi√©n puede ser descartada, s√≥lo que en este caso se requiere actualizar previamente la DRAM.
- Una l√≠nea que esta `Modified` o `Exclusive` se puede escribir desde la CPU en cualquier momento. En caso de que est√© `Exclusive`, pasa a `Modified`.
- En el caso en que se necesite escribir en una l√≠nea cuyo estado sea `Shared`, el protocolo indica que todas las dem√°s caches que tienen esa l√≠nea la invaliden previamente. Para ello se emplea una operaci√≥n broadcast denominada **Request For Ownership**.
- Un cache que tiene una l√≠nea en estado `Modified` y detecta por el Snoop BUS que alguien la lee, tambi√©n debe insertar el dato contenido por la l√≠nea, ya que est√° incoherente con la DRAM. Para ello realiza lo siguiente:
    - Activa la l√≠nea RFO, indic√°ndole a los dem√°s que ese dato est√° incoherente.
    - Escribe en la DRAM el valor actual de la l√≠nea. El lector copiar√° ese valor correcto a su cache cuando aparece en el BUS de datos.
    - Ambos pondr√°n la l√≠nea en `Shared`.
- Una l√≠nea en estado `Shared` pasa a `Invalid` cuando se recibe un RFO.

### Read For Ownership

- El protocolo de coherencia combina una escritura de una l√≠nea con un broadcast de invalidaci√≥n al resto de los controladores.
- Es enviada por un controlador cache si intenta escribir una l√≠nea que est√° en estado `Shared` o est√° en estado `Invalid`.
- Como resultado, el resto de los cache que tengan esa l√≠nea la tienen que invalidar.
- Desde el punto de vista del controlador cache, es una lectura de l√≠nea cacheada con toma de BUS del sistema para escribir el contenido de la l√≠nea en la memoria DRAM.

### Protocolo MESI - Diagrama de estados

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2015.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2015.png)

- PR: Processor Read (yo)
- BR: Bus Read (otro controlador lee, detecto un read en el bus)

## Arquitecturas de cache avanzadas

### Cache Multinivel

En alg√∫n momento de la evoluci√≥n de los procesadores, la tecnolog√≠a de integraci√≥n alcanz√≥ scalings que permitieron meter controladores cache + cache level 1 en un chip, y dejar una cache level 2 afuera del chip.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2016.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2016.png)

La cache level 1 la metieron adentro del chip del procesador para incrementarle la frecuencia. La cache level 2 estaba afuera del chip y su frecuencia ten√≠a que ser la del est√°ndar PCI. Adentro del chip, la frecuencia se pod√≠a duplicar.

### 2do. Nivel On chip

- Mete el cache level 2 tambi√©n dentro del chip.
- **Cache level 1 era como una arquitectura Harvard (separaba datos de instrucciones).**

### 3er. Nivel On chip

- Arranca con Intel Netburst.
- Mete el cache level 3 tambi√©n dentro del chip.

### Smart Cache

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2017.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2017.png)

# Paralelismo a Nivel de Instrucci√≥n

## Pipeline

### M√°quina de estados elemental

Una instrucci√≥n en un computador se ejecuta en etapas. Seg√∫n el modelo de Von Neumann, las etapas son: fetch, decode y execute. Hoy en d√≠a cada instrucci√≥n se puede desdoblar en m√°s etapas.

Antes del 80286, la mayor√≠a de los microprocesadores ejecutaban cada etapa en serie. Es decir, en el primer ciclo de clock, se ejecutaba una etapa de una instrucci√≥n, luego se ejecutaba la siguiente etapa de la misma instrucci√≥n, y as√≠ sucesivamente hasta haber ejecutado la √∫ltima etapa de esa instrucci√≥n, para luego empezar ejecutando la primer etapa de la siguiente instrucci√≥n.

![Ejecuci√≥n de una instrucci√≥n de forma serializada](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2018.png)

Ejecuci√≥n de una instrucci√≥n de forma serializada

**Problema:** Para ejecutar la siguiente instrucci√≥n hay que esperar a que la primer instrucci√≥n termine de ejecutarse completamente. No podr√≠amos ejecutar dos instrucciones a la vez?

### Pipeline

Arquitectura que permite crear el **efecto** de superponer en el tiempo la ejecuci√≥n de varias instrucciones a la vez con el objetivo de mejorar la performance.

**Requiere muy poco hardware adicional.**

**Idea:** Mientras se ejecu

**Ejemplo:**

5 *stages* (etapas):

1. Opcode fetch
2. Decode
3. Operand fetch
4. Execution
5. Retire result

Pipeline **ideal** para esas 5 etapas:

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2019.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2019.png)

**Primer ciclo:** Opcode fetch instr #1.

**Segundo ciclo:** Opcode fetch instr #2 + Decode instr #1.

**Tercer ciclo:** Opcode fetch instr #3 + Decode instr #2 + Operand fetch instr #1.

**Cuarto ciclo:** Opcode fetch instr #4 + Decode instr #3 + Operand fetch instr #2 + Exec instr #1.

**Quinto ciclo:** Opcode fetch instr #5 + Decode instr #4 + Operand fetch instr #3 + Exec instr #2 + Retire result instr #1. **Reci√©n ac√° se libera el primer resultado (el de la instrucci√≥n #1).**

**Conclusiones:**

- El pipeline demora tantos ciclos de clock en llegar al primer resultado como etapas tenga.
- En un pipeline de 5 etapas, en el caso de que cada etapa consuma solamente 1 ciclo de clock, obtenemos un resultado de instrucci√≥n por cada ciclo de clock a partir de la quinta etapa.
- Este escenario es te√≥rico y no responde a la situaci√≥n real.
- Agregar etapas parec√≠a mejorar la performance del pipeline.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2020.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2020.png)

### Hazards (obst√°culos)

Cuando hay un obst√°culo se denomina **pipeline stall (congesti√≥n del pipeline).**

**Obst√°culos estructurales:**

- Una etapa no est√° suficientemente atomizada, tiene muchas cosas que hacer y no se puede completar en un ciclo de clock (tarda mucho).
- Si dos instrucciones que van a utilizar esta etapa est√°n m√°s o menos pr√≥ximas en el tiempo, o est√°n a una distancia temporal mayor a la que la etapa consume para procesar, van a caer en un conflicto de recursos para su ejecuci√≥n. La que est√° primera en el pipeline va a tomar la etapa y cuando llegue la que ven√≠a en segundo lugar y quiera esa etapa del pipeline no va a poder.

**Ejemplo:**

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2021.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2021.png)

En el ciclo 5, la etapa de Opcode Fetch de la instrucci√≥n #5 y la etapa de Operand Fetch de la instrucci√≥n #3 quieren acceder a memoria a la vez. La soluci√≥n es elegir alguna de las dos y demorar la otra, corriendo todo el pipeline. Un criterio posible es priorizar la instrucci√≥n que este en la etapa m√°s avanzada. En este caso, la instrucci√≥n #3, la cual ya est√° en la etapa de Operand Fetch.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2022.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2022.png)

Luego, en el ciclo 8, tambi√©n hay un conflicto. Lo soluciono haciendo lo mismo (priorizando la instrucci√≥n #4, que la m√°s avanzada).

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2023.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2023.png)

Luego, en el ciclo 9, tambi√©n hay un conflicto. Lo soluciono haciendo lo mismo (priorizando la instrucci√≥n #5, que la m√°s avanzada).

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2024.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2024.png)

**Posibles soluciones:**

- **Cualquier soluci√≥n cuesta hardware**
- En el caso de accesos a memoria:
    - Podemos desdoblar el cache L1 en cache de datos y cache de instrucciones.
    - Emplear buffers de instrucciones implementados como peque√±as colas FIFO.
    - Ensanchamiento de los buses m√°s all√° de los anchos de palabra del procesador. Esto permite, por ejemplo, hacer que un opcode fetch se resuelva en un ciclo de clock. Si ensancho los buses, en un ciclo de clock leo mucha m√°s informaci√≥n. Es una forma de fetchear muchas instrucciones de una, o traerse muchos operandos de una. Tiene sentido por el principio de vecindad.
- Aumentar la capacidad del pipeline. Hacer etapas m√°s simples, capaces de resolver en un ciclo de clock su tarea.

**Obst√°culos de datos:**

- Una instrucci√≥n pide un dato antes de que √©ste est√© disponible por efecto de la secuencia l√≥gica que contiene el programa.

Ejemplo:

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2025.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2025.png)

La instrucci√≥n dsub depende del resultado de la instrucci√≥n dadd.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2026.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2026.png)

En el quinto ciclo, se genera el resultado de la suma, pero dicho resultado es requerido por `dsub`, `and`, `or` y `xor`. Las instrucciones `dsub` y `and` no pueden conseguir a tiempo el resultado en R1 para poder usarlo como operando.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2027.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2027.png)

**Posibles soluciones:**

- **Forwarding:**
    
    Retroalimento la ALU con la salida en el mismo ciclo que termina de ejecutarse la operaci√≥n aritm√©tica, as√≠, en el ciclo 5, la ALU ya tiene el operando correcto sin haber pasado por R1. No soluciona el problema del todo, pero se ahorra un ciclo.
    
    ![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2028.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2028.png)
    
    No siempre se puede hacer forwarding. Si la primer operaci√≥n no pasa por la ALU no se puede hacer forward.
    
    ![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2029.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2029.png)
    

**Obst√°culos de control:**

- Son causados por saltos.
- Un branch es lo peor que le puede pasar a un pipeline.
- El branch hace que todo lo que tenias preprocesado deba descartarse. Esto se conoce como **branch penalty**.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2030.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2030.png)

## Unidades de Predicci√≥n de saltos

### Necesidad de predicci√≥n de saltos

- Cuanto m√°s profundo es un pipeline, m√°s grande es la penalizaci√≥n de un branch. Ejemplo: para un pipeline de 5 etapas de 1 ciclo de clock cada una, la penalizaci√≥n es de 4 ciclos de clock.
- A medida que aumenta la complejidad del procesador, tambi√©n es l√≥gico que las etapas del pipeline aumenten (sencillamente porque el procesador es m√°s complicado).
- **En la pr√°ctica los Branch Predictors se implementan en la etapa de fetch.**

Predecir un salto es poder predecir el valor del program counter. Predecir un salto significa empezar a fetchear de una manera anticipada, para reducir el hueco en el pipeline. Hay situaciones en las que es imposible predecir.

### Asumir non-taken

**Asume por defecto que el salto nunca se toma.** Entonces el pipeline sigue fetcheando las instrucciones siguientes a la instrucci√≥n del branch. Es √∫til cuando el salto es hacia adelante. Por ejemplo:

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2031.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2031.png)

### Asumir taken

**Asume por defecto que el salto siempre se toma.** Entonces el pipeline empieza a fetchear las instrucciones que est√°n en la direcci√≥n de salto. Es √∫til en un loop donde se salta hacia atras. En general las iteraciones se hacen muchas veces, con lo cual los saltos son m√°s probables en esos casos. Por ejemplo:

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2032.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2032.png)

### Delayed branch

- Se introdujo en los primeros procesadores RISC.
- Tenemos N slots llamados Delay Slots.
- Las N instrucciones que vienen despu√©s de la instrucci√≥n de branch se ejecutan siempre, sin importar del camino tomado por el branch.
- Luego aplicas el resultado seg√∫n se necesite o no.
- Ejemplo con N=1:

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2031.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2031.png)

### Loop unrolling

**Es una tarea para el compilador.** Consiste en convertir los ciclos en instrucciones secuenciales, sin branches.

### Predicci√≥n de saltos din√°mica

Los m√©todos anteriores depend√≠an del compilador, del set de instrucciones. El hardware del procesador no hac√≠a ning√∫n an√°lisis de c√≥digo para analizar la instrucci√≥n.

**Idea:** Que ahora el procesador haga an√°lisis de flujo y tome decisiones acorde a lo que va viendo adelante de la instrucci√≥n de salto.

**Debemos predecir:**

1. Si la instrucci√≥n es de salto o no.
2. Si es taken o non-taken.
3. La direcci√≥n de salto (if taken).

**Branch Prediction Buffer (Last Time Predictor)**

- Tabla en cach√© que sirve para saber si el salto va a ser taken o non-taken.
- Se guarda como un bit extra en la BTB.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2033.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2033.png)

- Funciona bien para patrones tipo: TTTTTTTTTTTTTTTNNNNNNNNNNN.
- **El problema:** Funciona mal para patrones tipo TNTNTNTNTNTNTNTNTNT (0% accuracy en ese caso).
- **Soluci√≥n:** Armar una m√°quina de estados con dos bits.
- **Idea:** **No vamos a cambiar nuestra predicci√≥n al primer desacierto, la vamos a cambiar al segundo.**

**Si estamos en el estado de "Predice TAKEN (fuerte)" y la branch fue non-taken, pasamos al estado de "Predice TAKEN (debil)" en vez de pasar a un estado de "Predice Non-TAKEN".**

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2034.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2034.png)

**Branch Target Buffer**

- Tabla en cach√© que sirve para saber la direcci√≥n de salto (if taken).
- Cada entrada contiene la direcci√≥n de la instrucci√≥n de salto y la direcci√≥n target resuelta (en vez de los bits taken o non-taken, a diferencia de la Branch Prediction Buffer).
- Es m√°s r√°pido que el Branch Prediction Buffer (ya que ahora directamente podemos obtener la direcci√≥n de salto de esta tabla), pero el criterio sigue siendo el mismo.
- La proxima vez que nos topamos con el branch, la buscamos en la BTB. Si tenemos un hit, usamos la address guardada en la BTB para fetchear instrucciones.
- Si el valor no se encuentra en la BTB, se asume taken.
    - Si el resultado es Non-taken se acepta el delay en el pipeline y no almacena nada en la BTB.
    - Si el resultado es taken, ingresa el valor en la BTB.
- Si el valor se encuentra en la BTB, se aplica el campo de la direcci√≥n target almacenado.
    - Si el resultado es taken, no hay penalidad. No se guarda en el BTB ning√∫n nuevo valor ya que nos sirve.
    - Si el resultado es non-taken, sigue la m√°quina de estados de **Branch Prediction Buffer**.

## Superscalar

Consiste en paralelizar el pipeline, agregando otro.

**Ventaja:** Poder ejecutar m√°s de una instrucci√≥n por ciclo de clock, en paralelo.

### Superscalar de dos v√≠as

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2035.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2035.png)

El Pentium fue el primer procesador superscalar de la familia Intel.

### Problemas

- Los [obst√°culos estructurales](https://www.notion.so/Resumen-Final-Orga2-7e347eedef584449a5560e58e2399975) ahora quedan m√°s expuestos que antes. Ahora cada etapa, ademas de lidiar con las otras etapas de su propio pipeline (que pueden acceder en simult√°neo a la misma instrucci√≥n o dato), tambi√©n tiene que lidiar con las mismas etapas del pipeline de al lado, que tambi√©n pueden ir a buscar datos o c√≥digo. **Por eso los buses son muy importantes. Es importante separar buses de datos y address para minimizar los obst√°culos estructurales.**
- Cuantas m√°s vias tenga el pipeline, m√°s concurrencia de memoria.
- Si tengo dos ALUs, puedo ejecutar una instrucci√≥n en cada uno. En el caso de que una ALU dependa del resultado de la otra, esto no es posible.
- Una falla en el branch prediction limpia **todos** los pipelines.

## Scheduling Din√°mico

El Scheduling de instrucciones es la forma en que el procesador organiza la ejecuci√≥n de instrucciones. 

Hasta antes de superscalar, el scheduling era est√°tico. Ahora, con superscalar, el scheduling es din√°mico.

### Obst√°culos de Datos

Para los obst√°culos de Datos con superscalar, no podemos usar forwarding. El estudio de dependencias de datos se debe extender a instrucciones en los diferentes pipelines.

**Idea fuerza:**

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2036.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2036.png)

- La instrucci√≥n 2 (add.d) depende de la instrucci√≥n 1 (div.d), que para completarse requiere muchos ciclos de clock (debido a otros tipos de obst√°culos que afectan a la instrucci√≥n 1 pero no afectan a la instrucci√≥n 2).
- O sea, las instrucciones 2 y 3 no pueden ser ejecutadas hasta que la instrucci√≥n 1 termine.
- Esta obstrucci√≥n deja a todo el procesador sin uso hasta que la instrucci√≥n 1 termine.
- **La instrucci√≥n 3 (sub.d) no tiene ninguna dependencia con las instrucciones previas, sin embargo, la instrucci√≥n queda esperando.**

**Soluci√≥n: Ejecuci√≥n fuera de orden. Ejecuto la instrucci√≥n 3 igual, sin esperar.** 

# Ejecuci√≥n Fuera de Orden

## Conceptos fundamentales

### Idea fuerza

- Enviar las instrucciones a ejecuci√≥n independientemente del orden que tengan en el c√≥digo. Con qu√© criterio?
- Una vez que una instrucci√≥n es decodificada, puedo saber si esa instrucci√≥n tiene atascos (estructurales, de datos, etc).

### Ejecuci√≥n Fuera de Orden vs En Orden

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2037.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2037.png)

- Separamos la etapa de resultado en resultado y escritura.
- Si bien vamos a ejecutar fuera de orden, no podemos aplicar los resultados de las operaciones en un orden distinto al que est√° establecido en el programa porque sencillamente alterar√≠amos la l√≥gica del programa y eso es inadmisible (por algo las instrucciones est√°n en ese orden).
- Como las instrucciones est√°n atomizadas en distintas etapas, lo que vamos a tratar de hacer es que una parte de ese pipeline trabaje fuera de orden, y otra parte trabajen en orden.
- En general, la etapa de ejecuci√≥n es la etapa que se hace fuera de orden.
- La etapa de write va en orden para no alterar el comportamiento.

### Riesgos

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2038.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2038.png)

- **Write, After Read (WAR):**
    
    La instrucci√≥n 3 escribe F8 y despu√©s la instrucci√≥n 2 lee F8. **La operaci√≥n de la instrucci√≥n 2 tiene un operando inv√°lido.**
    
- **Write, After Write (WAW):**
    
    La instrucci√≥n 4 escribe F6 y despu√©s la instrucci√≥n 2 escribe F6. **Me quedo con un valor m√°s viejo de F6 en la instrucci√≥n 5.**
    
- **Read, After Write (RAW):**
    
    La instrucci√≥n 2 lee F0 y despu√©s la instrucci√≥n 1 escribe F0. **Este es el cl√°sico obst√°culo de datos que vimos al principio, antes de superscalar. Se soluciona stalleando el pipeline.**
    

### Excepciones imprecisas

- Las operaciones generan excepciones, que a su vez generan interrupciones para resolver las excepciones.
- Si estamos ejecutando fuera de orden una instrucci√≥n que genera una excepci√≥n, no deber√≠amos generar la excepci√≥n fuera de orden, ya que eso limpiar√≠a el pipeline para saltar a la interrupci√≥n.
- Se debe preservar ese escenario.
- Tendr√≠amos que poder "bufferear" el estado de esa excepci√≥n.

## M√©todo prehistorico

### Scoreboarding

- Es el primer m√©todo que se desarroll√≥ para ejecutar fuera de orden.
- Es el menos sofisticado.
- Se implement√≥ en la CDC 6600 en **1964**.
- No se usa.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2039.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2039.png)

- Una Unidad de B√∫squeda arma una cola de preb√∫squeda que contiene las pr√≥ximas instrucciones.
- La Unidad de Env√≠o toma lo que hay en la cola de preb√∫squeda y lo pone en un Scoreboard de capacidad 4.
- La Unidad de Lectura de Operando se fija si los operandos de las instrucciones en el Scoreboard estaban disponibles y en funci√≥n de eso las despacha a las Unidades Funcionales.

**Desventajas:**

- Los operandos se leen directamente en los registros, as√≠ que no se pod√≠a hacer forwarding. No ten√≠a una lectura prolija de los operandos.

## M√©todo actual

### Modelo de Tomasulo

- 1967: Tomasulo present√≥ un trabajo de Scheduling din√°mico en la unidad de punto flotante del mainframe IBM 360/91.
- **Trata de minimizar los riesgos RAW.**
- **Neutraliza los riesgos WAR y WAW.**

**Tomasulo se pregunt√≥ qu√© necesita un procesador para poder ejecutar fuera de orden?** 

1. **Mantener un "enlace" entre quien produce un dato y quien o quieres lo consumen. La l√≥gica del procesador tiene que tener claro, para cada operando destino, qui√©n lo va a utilizar en las instrucciones posteriores.**
2. **Mantener en espera las instrucciones que todav√≠a no se puedan ejecutar, es decir, que tengan al menos un operando en espera.**
3. **Las instrucciones tienen que saber cu√°ndo est√°n disponibles sus operandos.**
4. **Ejecutar la instrucci√≥n cuando todos sus operandos est√©n "Ready".**

### **Register Alias Table (RAT)**

- **Soluciona el problema de [mantener un enlace entre el productor de un dato y sus consumidores](https://www.notion.so/Resumen-Final-Orga2-7e347eedef584449a5560e58e2399975).**
- A cada operando, se le asocia un alias y se indica si es v√°lido o no.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2040.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2040.png)

- **Tag:** Alias/renombre del registro.
- **Valor:** Valor del registro.
- **Validez:** Dice si el valor es v√°lido o no. Si es inv√°lido significa que otra instrucci√≥n todav√≠a no escribi√≥ el nuevo valor de este registro.

**Ejemplo:**

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2041.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2041.png)

- La instrucci√≥n 2 tiene un riesgo WAR en F8 con la instrucci√≥n 4 .
- La instrucci√≥n 5 tiene un riesgo WAR en F6 con la instrucci√≥n 2.
- Reemplazamos los operandos problem√°ticos por sus respectivos aliases (S y T).

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2042.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2042.png)

### **Reservation Station (RS)**

- **Soluciona los otros tres problemas del algoritmo de Tomasulo.**
    - **Soluciona el problema de [mantener en espera las instrucciones que todav√≠a no se pueden ejecutar](https://www.notion.so/Resumen-Final-Orga2-7e347eedef584449a5560e58e2399975).**
    - **Soluciona el problema de [saber cu√°ndo est√°n disponibles los operandos de una instrucci√≥n](https://www.notion.so/Resumen-Final-Orga2-7e347eedef584449a5560e58e2399975).**
    - **Soluciona el problema de [ejecutar la instrucci√≥n cuando todos sus operandos est√©n disponibles](https://www.notion.so/Resumen-Final-Orga2-7e347eedef584449a5560e58e2399975).**
- Cada l√≠nea de la RS es una instrucci√≥n y sus dos operandos.
- La RS tiene un tama√±o limitado.
- Una vez realizado el renombre se le asigna, a la instrucci√≥n, una Reservation Station.
- Una Reservation Station es un subsistema de hardware compuesto de bancos de registros internos que se encarga de mantener las instrucciones en espera hasta que est√©n listas para ser ejecutadas. √âste chequea constantemente por la disponibilidad de los operandos de la instrucci√≥n. Para cada uno de ellos cuyo valor no est√© disponible, la RS guarda el tag que se le asign√≥ en el paso anterior.
- Cada vez que una unidad de ejecuci√≥n pone Ready un operando, transmite su tag asociado junto con su valor a todas las RS. Cuando una instrucci√≥n tiene todos  sus operandos Ready, la RS espera a que la unidad funcional asociada a ella est√© libre y luego la despacha.
    
    ![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled_Diagram_(1).png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled_Diagram_(1).png)
    

### **Common Data Bus:**

- Es un datapath que va por toda la arquitectura.
- **Se usa para broadcastear el tag y value que salen de las ALU hacia la Register Alias Table y las RS para que tengan esos valores actualizados.**

### FPU de [IBM 360/91](https://www.notion.so/Resumen-Final-Orga2-7e347eedef584449a5560e58e2399975) con la mejora de Tomasulo

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2043.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2043.png)

### Algoritmo de Tomasulo

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2044.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2044.png)

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2045.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2045.png)

**El algoritmo se ejecuta para cada instrucci√≥n del programa.**

// Defino si meto la instrucci√≥n en la RS o si stalleo el pipeline

- Chequeamos que haya lugar en la RS para todos los operandos de la instrucci√≥n (tanto fuentes como destino).
    
    // Hay lugar en la RS
    
    1. Creo un alias para cada operando no v√°lido de la instrucci√≥n
    2. Insertamos la instrucci√≥n y sus operandos renombrados en la RS. Estos valores los sacamos directamente de la Register Alias Table.
    
- Si no hay lugar, stall

// La instrucci√≥n ya est√° en la RS

- Cada instrucci√≥n almacenada en la RS hace:
    1. Chequear el Common Data Bus en busca de tags que correspondan a sus operandos fuente.
    2. Cuando se detecta el tag que se esta esperando, se graba el valor de la fuente en la RS.
    3. Cuando ambos operandos estan Ready, la instrucci√≥n se marca Ready para ser ejecutada.

// La instrucci√≥n ya est√° lista para ser ejecutada

- Me fijo si hay alguna Unidad Funcional disponible (quien ejecuta la instrucci√≥n)
    
    // Hay una Unidad Funcional disponible
    
    1. Despacho la instrucci√≥n
    
    **// Listo, se despach√≥ la instrucci√≥n ‚úÖ**
    

- Me fijo si finaliz√≥ la ejecuci√≥n de la instrucci√≥n
    
    // Finaliz√≥ la ejecuci√≥n de la instrucci√≥n
    
    1. La Unidad Funcional que ejecut√≥ la instrucci√≥n broadcastea, usando el Common Data Bus, el tag y valor resultante.
        
        // Esto significa que el operando destino de esta instrucci√≥n ahora est√° Ready. Entonces se lo comunico a los dem√°s v√≠a broadcast
        
        // A la Register Alias Table le llega, por broadcast, el tag y valor
        
    2. Si tiene ese tag guardado:
        1. Escribe el valor que le llega en el registro (R1, R2, etc), seg√∫n corresponda con el tag.
        2. Marca ese tag como v√°lido.
        3. Libera el tag.

### Ejemplo del Algoritmo

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2046.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2046.png)

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2047.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2047.png)

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2048.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2048.png)

## Especulaci√≥n por Hardware

### Reordenando los resultados

- En el modelo de Tomasulo no hay ning√∫n bloque que asegure que los resultados se escriben en orden.
- **M√°s all√° de que las instrucciones puedan ejecutarse en paralelo, la aplicaci√≥n de los resultados necesariamente debe ser en orden.**
- Las implementaciones posteriores introdujeron un m√≥dulo para hacer eso que se llama ReOrder Buffer (ROB).
    - Es un bloque parecido a la Reservation Station de Tomasulo. Contiene los valores de resultado de la etapa de ejecuci√≥n y a qu√© registros deben ir.
    - El resultado permanece en el ROB desde que se obtenga hasta que se copie en el operando destino.
    - Contiene los campos `Tipo de instrucci√≥n`, `Destino`, `Valor` y `Ready`.
    - Escribe un valor en su destino si los anteriores valores est√°n en ready o ya fueron aplicados. Se pueden escribir varios valores en el mismo ciclo si est√°n todos en ready.

# Casos pr√°cticos que mezclan todo lo visto

## Pentium Pro

- Fue un procesador muy importante dentro de la genealog√≠a de Intel. Inaugur√≥ la microarquitectura P6, cuyo fundamento de ejecuci√≥n fuera de orden se mantiene hasta la fecha.
- Fue el primer procesador de Intel que ejecutaba fuera de orden. Los Pentium ejecutaban en orden. Los Pentium Pro ejecutaban fuera de orden.
- Luego vinieron el Pentium II, Pentium II Xeon, Celeron, Pentium III y Pentium III Xeon.
- Todos estos se basaban en una estructura interna denominada Three Cores Engine.

### Three Cores Engine

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2049.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2049.png)

- **Tres cores:**
    1. **Unidad de decodificaci√≥n y b√∫squeda (Fetch/Decode Unit)**
    2. **Unidad de despacho y ejecuci√≥n (Dispatch/Execute Unit)**
    3. **Unidad de retiro (Retire Unit)**
- Cache de datos y cache de c√≥digo separadas
- Scheduling din√°mico
- Basado en una ventana de instrucciones y no en un pipeline superescalar
- **Las instrucciones se traducen en microoperaciones b√°sicas.**
- **Las microoperaciones se almacenan en el instruction pool (que no es m√°s que un reorder buffer).**
- Los tres cores tienen visibilidad de esa ventana de ejecuci√≥n.
- La ejecuci√≥n fuera de orden y especulativa es para las microoperaciones.
- Las microoperaciones ten√≠an todas el mismo tama√±o, como si fuera RISC.

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2050.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2050.png)

### Unidad de Interfaz con el Bus

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2051.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2051.png)

- Es un bloque que trabaja parcialmente en orden.
- Est√° encargada de conectar los tres cores con el mundo exterior.
- Divide los modelos:
    - De la Unidad de Interfaz hacia afuera (incluyendo la Cache L2 afuera), es Von Neumann.
    - De la Unidad de Interfaz hacia adentro es Harvard. La Cache L1 est√° separada en datos y c√≥digo.
- Soporta hasta cuatro accesos concurrentes a la Cache L2.
- El acceso al Bus del sistema se regula todo por MESI y snooping para asegurar la coherencia de la memoria.

### Unidad de decodificaci√≥n y b√∫squeda

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2052.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2052.png)

- **Instruction Cache L1:** tiene 16 bytes por l√≠nea. Entrega los 16 bytes alineados a la Unidad de Decodificaci√≥n.
- **Puntero Next IP:** Apunta a la l√≠nea donde est√° la siguiente instrucci√≥n o a la de la instrucci√≥n target de un salto. Es decir, indica qu√© instrucci√≥n tiene que ir fetcheando el pipeline.
- **Tres Instruction Decoders:** Dos de instrucciones simples y uno de instrucciones complejas.
    - Los decodificadores de instrucciones simples traducen la instrucci√≥n en una microoperaci√≥n
    - El decodificador de instrucciones complejas traduce la instrucci√≥n en dos o m√°s microoperaciones.
- **Microcode Instruction Sequencer:** Convierte una instrucci√≥n todav√≠a m√°s compleja en una secuencia de microoperaciones.

### Unidad de despacho y ejecuci√≥n

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2053.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2053.png)

- **Esta unidad es la √∫nica que trabaja fuera de orden.**
- Selecciona las microoperaciones dependiendo del estado (no del orden) desde el pool de instrucciones.
    - Si el estado de la microoperaci√≥n indica que sus operandos est√°n disponibles, la Reservation Station verifica que alguna unidad funcional est√© libre para su ejecuci√≥n y le env√≠a la microoperaci√≥n.
- El resultado se escribe en el pool de operaciones (ReOrder Buffer).
- **Branches:** Las microoperaciones se marcan como saltos en el ReOrder Buffer y tambi√©n se escribe all√≠ la direcci√≥n predicta por el BTB (Branch Target Buffer). Si el salto falla, limpia el ROB, las RS (limpia todo).

### Unidad de retiro

![Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2054.png](Resumen%20Final%20Orga2%20a7de9775164c482cad5121a79d2a7fdc/Untitled%2054.png)

- Mira permanentemente el estado de cada instrucci√≥n en el ROB.
- No s√≥lo chequea su estado, sino que las reinserta en el orden en que ocupan en el programa.
- Esto incluye el procesamiento de interrupciones, excepciones, traps, breakpoints y predicciones fallidas.
- **Retirement Register File:** Se encarga de escribir los operandos resultantes en los registros de la arquitectura.
- **Unidad de interfaz de memoria:** Aplica los datos en el Cache L1 de datos.

## Intel NetBurst (Pentium 4)

- Es el siguiente en la l√≠nea que marc√≥ una cierta innovaci√≥n.
- En vez de Cache L1 de c√≥digo ten√≠a un **Trace Cache:**
    - Es una cache que guarda microoperaciones en vez de instrucciones.

# Procesadores Multithread

- Los procesadores multithread tienen dos estados arquitecturales.
- **No son dos procesadores, sino que son dos vistas de procesadores.**
- **Se duplica lo necesario y se comparte lo cr√≠tico.**
- Se duplica:
    - La cola de microoperaciones
    - El Instruction Pointer
    - Register Alias Table
    - El ROB
- Se comparte:
    - La cache L1 de datos
    - Las unidades de ejecuci√≥n
    - Los registros internos de la arquitectura
    - Las Reservation Station
    - etc
- **Conclusi√≥n:** La mayor√≠a de cosas se comparte. Con un 15% m√°s de electr√≥nica se logra un aumento mucho mayor a 15% en la performance.

# Multicore y Manycore

- **Es poner dos procesadores completos dentro del chip.**
- Dejaron obsoletos a los procesadores multithread.
- **Es mejor muchos cores sencillos que un s√≥lo core superpoderoso. Con m√°s cores tenemos igual performance y menor consumo.**